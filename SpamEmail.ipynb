{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "SpamEmail.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDmRHwAhpccc",
        "outputId": "9980e5b8-eecd-475c-f3e0-08e8e6772359"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k72njJP_pccf"
      },
      "source": [
        "df = pd.read_csv('/content/sample_data/bayes/spam.csv', encoding='ISO-8859-1')\n",
        "le = LabelEncoder()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBbSauSzpccj"
      },
      "source": [
        "data = df.to_numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb_5X1Espccl"
      },
      "source": [
        "X = data[:, 1]\n",
        "y = data[:, 0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0qXmTtApccm",
        "outputId": "ab62ee79-5a2c-4a06-8f29-460c51b6b942"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5572,), (5572,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HlUcvdQpccp"
      },
      "source": [
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "sw = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7u6yReFpccq"
      },
      "source": [
        "def getStem(review):\n",
        "    review = review.lower()\n",
        "    tokens = tokenizer.tokenize(review) # breaking into small words\n",
        "    removed_stopwords = [w for w in tokens if w not in sw]\n",
        "    stemmed_words = [ps.stem(token) for token in removed_stopwords]\n",
        "    clean_review = ' '.join(stemmed_words)\n",
        "    return clean_review"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNWFkM_ypccr"
      },
      "source": [
        "# get a clean document\n",
        "def getDoc(document):\n",
        "    d = []\n",
        "    for doc in document:\n",
        "        d.append(getStem(doc))\n",
        "    return d"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UTuRh5cpcct"
      },
      "source": [
        "stemmed_doc = getDoc(X)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqKDIxH1pccu",
        "outputId": "b2b18fb3-6611-4dbd-a4b7-c047bfd52ed8"
      },
      "source": [
        "stemmed_doc[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
              " 'ok lar joke wif u oni',\n",
              " 'free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri question std txt rate c appli 08452810075over18',\n",
              " 'u dun say earli hor u c alreadi say',\n",
              " 'nah think goe usf live around though',\n",
              " 'freemsg hey darl 3 week word back like fun still tb ok xxx std chg send å 1 50 rcv',\n",
              " 'even brother like speak treat like aid patent',\n",
              " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend callertun',\n",
              " 'winner valu network custom select receivea å 900 prize reward claim call 09061701461 claim code kl341 valid 12 hour',\n",
              " 'mobil 11 month u r entitl updat latest colour mobil camera free call mobil updat co free 08002986030']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Limoguvmpccv"
      },
      "source": [
        "cv = CountVectorizer()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJq2P5Olpccw"
      },
      "source": [
        "# create my vocab\n",
        "vc = cv.fit_transform(stemmed_doc)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W6LMkTDpccx"
      },
      "source": [
        "X = vc.todense()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNNV0XXpccy"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "...     X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hjkXc5upcc0"
      },
      "source": [
        "# NB from sklearn"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrLoqGDkpcc1"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEKKXCxEpcc2",
        "outputId": "1473fab7-2b18-4e2f-a50b-8b43448623f1"
      },
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.977705274605764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mraCCuhGpcc3"
      },
      "source": [
        "messages = [\n",
        "    \"\"\"\n",
        "    Hi anonymous,\n",
        "We invite you to participate in MishMash - India’s largest online diversity hackathon. \n",
        "The hackathon is a Skillenza initiative and sponsored by Microsoft, Unity, Unilever, Gojek, Rocketium and Jharkhand Government. \n",
        "We have a special theme for you - Deep Tech/Machine Learning - sponsored by Unilever, which will be perfect for you.\n",
        "    \"\"\",\n",
        "    \"\"\"Join us today at 12:00 PM ET / 16:00 UTC for a Red Hat DevNation tech talk on AWS Lambda and serverless Java with Bill Burke.\n",
        "Have you ever tried Java on AWS Lambda but found that the cold-start latency and memory usage were far too high? \n",
        "In this session, we will show how we optimized Java for serverless applications by leveraging GraalVM with Quarkus to \n",
        "provide both supersonic startup speed and a subatomic memory footprint.\"\"\",\n",
        "\n",
        "    \"\"\"We really appreciate your interest and wanted to let you know that we have received your application.\n",
        "There is strong competition for jobs at Intel, and we receive many applications. As a result, it may take some time to get back to you.\n",
        "Whether or not this position ends up being a fit, we will keep your information per data retention policies, \n",
        "so we can contact you for other positions that align to your experience and skill set.\n",
        "\"\"\",\n",
        " \"\"\"\n",
        "    1 year of Free Netflix? Who can say no to that!\n",
        "\n",
        "Stop \"borrowing\" your friend's Netflix account! Lock your phone this week, collect gems and stand a chance to win 1 year of FREE NETFLIX on us!\n",
        "\n",
        "Click here to start locking your phone right NOW!\n",
        "\n",
        "Special Shoutout to Alok from Patna, India on winning a Samsung Smartwatch for last week's #SOTW competition! You can be next week's winner by simply locking your phone!\n",
        "Did you watch the FRIENDS reunion?\n",
        "\n",
        "Last week, our favorite friends came together on that Central Perk couch for what was probably the last time and team Lock&Stock is still trying to get over the rush of emotions we felt. We now finally know that Ross and Rachel were indeed on a break and that of the billions of times the show has been watched, a few members of the L&S team are probably responsible for half of it.\n",
        "\n",
        "Let us know what you thought of the episode by DMing us or responding to this email!\n",
        "\n",
        "P.S. for all those of you who didn't like the episode, we've just got one thing to say\n",
        "\"\"\"\n",
        "]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqvjt7Srpcc5"
      },
      "source": [
        "def prepare(messages):\n",
        "    d = getDoc(messages)\n",
        "    # dont do fit_transform!! it will create new vocab.\n",
        "    return cv.transform(d)\n",
        "\n",
        "messages = prepare(messages)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arnmGatSpcc5",
        "outputId": "d955afdd-e7b4-4c51-aea8-3f915b73154b"
      },
      "source": [
        "y_pred = model.predict(messages)\n",
        "y_pred"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ham', 'spam', 'ham', 'ham'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wACMWcxvIMu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}